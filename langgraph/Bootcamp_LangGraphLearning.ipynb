{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktLysPA_8ql1"
      },
      "source": [
        "# Building the simplest Graph\n",
        "\n",
        "We start with a graph with two nodes connected by one edge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VypRM0wh8ql3",
        "outputId": "99095949-3168-4023-a12c-f3022c2553ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/150.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m143.4/150.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.0/150.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install  -qU langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0LsYw1F8ql5"
      },
      "source": [
        "Nodes act like functions that can be called as needed. In our case Node 1 is our starting point and Node 2 is our finish point."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Nodes**\n",
        "Nodes are just python functions.\n"
      ],
      "metadata": {
        "id": "EqvMj62dwA6v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g3I6ovHC8ql5"
      },
      "outputs": [],
      "source": [
        "def function_1(input_1):\n",
        "    return input_1 + \" I am \"\n",
        "\n",
        "def function_2(input_2):\n",
        "    return input_2 + \"happy\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Edges**\n",
        "Edges connect the nodes.\n",
        "\n",
        "Normal Edges are used if you want to always go from, for example, node_1 to node_2."
      ],
      "metadata": {
        "id": "nEk2pwAyvvqd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jIs2ZzLq8ql6"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph, START, END\n",
        "\n",
        "# Define a graph\n",
        "workflow = Graph()\n",
        "\n",
        "# define nodes\n",
        "workflow.add_node(\"node_1\", function_1)\n",
        "workflow.add_node(\"node_2\", function_2)\n",
        "\n",
        "# define edges\n",
        "workflow.add_edge(START, 'node_1')\n",
        "workflow.add_edge('node_1', 'node_2')\n",
        "workflow.add_edge('node_2', END)\n",
        "\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph)\n",
        "print(type(graph))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWOgXPFO9rKk",
        "outputId": "1aaec4c8-0666-4588-9787-c23b917e7960"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<langgraph.graph.graph.CompiledGraph object at 0x7ef18a311610>\n",
            "<class 'langgraph.graph.graph.CompiledGraph'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(graph.get_graph())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axGzMlCQ91DL",
        "outputId": "cf3ffd4c-d45b-45d3-81ce-ddd7fe0148e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph(nodes={'__start__': Node(id='__start__', name='__start__', data=<class 'langgraph.graph.graph.LangGraphInput'>, metadata=None), 'node_1': Node(id='node_1', name='node_1', data=node_1(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), 'node_2': Node(id='node_2', name='node_2', data=node_2(tags=None, recurse=True, explode_args=False, func_accepts_config=False, func_accepts={}), metadata=None), '__end__': Node(id='__end__', name='__end__', data=<class 'langgraph.graph.graph.LangGraphOutput'>, metadata=None)}, edges=[Edge(source='__start__', target='node_1', data=None, conditional=False), Edge(source='node_1', target='node_2', data=None, conditional=False), Edge(source='node_2', target='__end__', data=None, conditional=False)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "JpkrxJ5H-Fg5",
        "outputId": "99bb9c02-fec5-436c-f584-7dd34432316a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executing a LangGraph: The Runnable Protocol\n",
        "\n",
        "When a LangGraph is compiled, it **implements the Runnable Protocol**.  \n",
        "This means it follows a **standardized way** of execution in LangChain.\n",
        "\n",
        "### 🔹 What is the Runnable Protocol?\n",
        "It defines methods to run LangChain components, including:\n",
        "- `.invoke(input)`: Runs the component with an input and returns the result.\n",
        "- `.stream(input)`: Streams the output in real-time.\n",
        "\n",
        "### 🔹 Why is this important?\n",
        "Since the compiled graph follows this protocol, we can execute it just like other LangChain components.\n",
        "\n",
        "### 🔹 Example Usage\n",
        "```python\n",
        "output = graph.invoke(input)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "EP3-EmV7B50O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nXLvJuFV8ql6",
        "outputId": "6607e7d5-4b17-4cf8-92c7-2955fc1595e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello I am happy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "graph.invoke(\"Hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**invoke** runs the entire graph synchronously.\n",
        "\n",
        "This waits for each step to complete before moving to the next.\n",
        "\n",
        "It returns the final state of the graph after all nodes have executed"
      ],
      "metadata": {
        "id": "3UU5J4dgC2QF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I02yJ3u18ql7",
        "outputId": "906d6a59-c511-496a-f081-191fdd8859c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from node 'node_1':\n",
            "---\n",
            "Hello I am \n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'node_2':\n",
            "---\n",
            "Hello I am happy\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = 'Hello'\n",
        "for output in graph.stream(input):\n",
        "    # print(output)\n",
        "\n",
        "    for key,value in output.items():\n",
        "      # print(item)\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG__FIOo8ql7"
      },
      "source": [
        "### As you can see, we can run the nodes as functions and return some values from them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h96gjzRi8ql7"
      },
      "source": [
        "# Adding LLM Call\n",
        "\n",
        "Now, let's make the first node as an \"Agent\" that can call Open AI models. We can use langchain to make this call easy for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1VUZvmOs8ql7",
        "outputId": "ba5151c9-6138-482b-aa97-761d08aa5d42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/413.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Srh77XVn8ql8"
      },
      "outputs": [],
      "source": [
        "# access environment variables\n",
        "\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AUtuSLRD8ql8"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize an instance of the ChatGoogleGenerativeAI with specific parameters\n",
        "model: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    # model=\"gemini-1.5-flash\",  # Specify the model to use\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    api_key=gemini_api_key    # Provide the Google API key for authentication\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Iv7QykoU8ql8",
        "outputId": "1277ecec-f162-473c-c421-000b06babe40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hey! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-18a31962-bb85-49e2-b479-ebd78c33f89a-0', usage_metadata={'input_tokens': 2, 'output_tokens': 10, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#Call the model with a user message\n",
        "response = model.invoke('Hey there')\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke('Hey there')\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YIJk_UOPGq1n",
        "outputId": "0d5d2682-06e5-4578-c539-61fe4238175b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hey! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5dZrQih8ql9"
      },
      "source": [
        "Cool! Keeping that in mind, let's change the function 1 above so that we can send the user question to the model. Then we will send this response to function 2, which will add a short string and return to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7urgwH3L8ql9"
      },
      "outputs": [],
      "source": [
        "def function_1(input_1):\n",
        "    response = model.invoke(input_1)\n",
        "    return response.content\n",
        "\n",
        "def function_2(input_2):\n",
        "    return \"Agent Says: \" + input_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z1eqPVym8ql9"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph, START, END\n",
        "\n",
        "# Define a graph\n",
        "workflow = Graph()\n",
        "\n",
        "# define nodes\n",
        "workflow.add_node(\"node_1\", function_1)\n",
        "workflow.add_node(\"node_2\", function_2)\n",
        "\n",
        "# define edges\n",
        "workflow.add_edge(START, 'node_1')\n",
        "workflow.add_edge('node_1', 'node_2')\n",
        "workflow.add_edge('node_2', END)\n",
        "\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "SusYj_sT1e0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "F53jEPAr8ql9",
        "outputId": "d019b259-8e6b-43d4-806b-7366cb5ea28c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agent Says: Hey! How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "graph.invoke(\"Hey there\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just a helpter function\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def to_markdown(text)-> Markdown:\n",
        "    text : str = text.replace(\"•\", \"  *\")\n",
        "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "DIhrkUUgMCx-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response  = graph.invoke(\"Motivate me to learn agentic ai\")\n",
        "\n",
        "# print(response)\n",
        "to_markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "hKmFL98fMFmf",
        "outputId": "45cc7ded-87d7-4847-f5f2-752f4292a8fd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Agent Says: Okay, let's get you fired up about learning Agentic AI! Here's a motivational cocktail, blending the potential, the impact, and the sheer coolness of this emerging field:\n> \n> **1. The Future is Autonomous (and You Can Build It):**\n> \n> *   **Imagine:** AI that doesn't just passively respond to commands, but proactively tackles complex problems, learns from its mistakes, and achieves goals independently.  That's the promise of agentic AI. It's not just a \"nice-to-have\" feature; it's a fundamental shift in how we interact with and leverage AI.  **By learning Agentic AI, you're positioning yourself at the forefront of this technological revolution.**\n> *   **Solve Real-World Problems:** Think about automating scientific discovery, optimizing complex supply chains, personalizing education at scale, or creating truly intelligent personal assistants.  Agentic AI can tackle problems that are currently too complex or time-consuming for humans alone.  **You can be the one to build solutions that have a tangible and positive impact on the world.**\n> \n> **2. Unlock Limitless Career Opportunities:**\n> \n> *   **High Demand, Low Supply:** The skills to build, deploy, and manage agentic AI systems are in incredibly high demand.  As companies across industries recognize the potential, they're scrambling for talent.  **Learning Agentic AI gives you a competitive edge and opens doors to exciting and lucrative career paths.**\n> *   **Shape the Industry:** This field is still nascent, meaning there's ample opportunity to define best practices, contribute to open-source projects, and become a thought leader.  **You can be a pioneer, shaping the future of Agentic AI and influencing its development.**\n> *   **Diverse Roles:**  Whether you're interested in research, development, engineering, product management, or entrepreneurship, Agentic AI offers a wide range of roles to suit your skills and interests.\n> \n> **3. The Sheer Intellectual Challenge and Fun:**\n> \n> *   **Cutting-Edge Technology:** You'll be working with the latest advancements in AI, including large language models (LLMs), reinforcement learning, planning algorithms, and more. **It's a constant learning experience that will keep your mind engaged and challenged.**\n> *   **Creative Problem-Solving:**  Building agentic AI systems requires creative problem-solving.  You'll be designing architectures, defining goals, and developing strategies to enable AI to act autonomously and achieve its objectives.  **It's like building a digital mind, which is incredibly rewarding.**\n> *   **Tangible Results:**  Seeing an AI agent successfully navigate a complex environment or solve a challenging problem is incredibly satisfying.  **You'll get to witness the power of your creations in action.**\n> \n> **4. Breaking Down the Intimidation Factor:**\n> \n> *   **Start Small, Grow Big:** You don't need to become an expert overnight. Start with the fundamentals, focus on a specific application, and gradually expand your knowledge. **There are plenty of resources available to guide you on your journey.**\n> *   **Community Support:** The Agentic AI community is growing rapidly.  You'll find plenty of online forums, tutorials, and open-source projects to help you learn and connect with other enthusiasts.  **You're not alone on this journey!**\n> *   **Focus on Practical Application:** Don't get bogged down in theory.  Focus on building simple agents and experimenting with different techniques. **Hands-on experience is the best way to learn and solidify your understanding.**\n> \n> **Here's a concrete action plan to get you started:**\n> \n> 1.  **Define Your \"Why\":**  What specific problem do you want to solve with Agentic AI?  Having a clear goal will keep you motivated.\n> 2.  **Start with the Basics:**  Learn the fundamentals of AI, machine learning, and programming (Python is a great choice).\n> 3.  **Explore LLMs:** Dive into the world of Large Language Models (LLMs) like GPT-3, GPT-4, or open-source alternatives. Understand how they can be used as the \"brain\" of an agent.\n> 4.  **Study Agentic AI Frameworks:**  Explore frameworks like LangChain, AutoGPT, or BabyAGI to understand how to structure your agentic systems.\n> 5.  **Build a Simple Agent:**  Start with a small project, like an agent that can summarize articles or answer questions about a specific topic.\n> 6.  **Join the Community:**  Engage with other learners and experts in online forums, attend webinars, and contribute to open-source projects.\n> 7.  **Stay Curious:**  Keep up with the latest research and developments in Agentic AI.\n> \n> **Final thought:**  Agentic AI is not just a technology; it's a paradigm shift.  By learning it, you're not just acquiring a skill; you're investing in your future and positioning yourself to be a leader in the age of intelligent machines.  **Go build something amazing!**"
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P1P5e0Mb8ql-",
        "outputId": "28f2e698-abc9-4539-a39f-b9eb83beb7c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from node 'node_1':\n",
            "---\n",
            "Hey! How's it going? What can I help you with today?\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'node_2':\n",
            "---\n",
            "Agent Says: Hey! How's it going? What can I help you with today?\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = 'Hey there'\n",
        "for output in graph.stream(input):\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPM0sRit8ql-"
      },
      "source": [
        "# First functional Agent App - City Temperature\n",
        "\n",
        "### Step 1: Parse the city mentioned\n",
        "Let's extract the city that a user mentions in a query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ed2GvzXz8ql-"
      },
      "outputs": [],
      "source": [
        "def function_1(input_1):\n",
        "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
        "        Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
        "    response = model.invoke(complete_query)\n",
        "    return response.content\n",
        "\n",
        "def function_2(input_2):\n",
        "    return \"Agent Says: \" + input_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "pAkoHhCg8ql-"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph, START, END\n",
        "\n",
        "# Define a graph\n",
        "workflow = Graph()\n",
        "\n",
        "# define nodes\n",
        "workflow.add_node(\"node_1\", function_1)\n",
        "workflow.add_node(\"node_2\", function_2)\n",
        "\n",
        "# define edges\n",
        "workflow.add_edge(START, 'node_1')\n",
        "workflow.add_edge('node_1', 'node_2')\n",
        "workflow.add_edge('node_2', END)\n",
        "\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "pqVBGxGB1iIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LYJRVOLL8ql_",
        "outputId": "5dba4c88-2e67-4ca5-c746-39ae0d521643",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Agent Says: Peshawar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "graph.invoke(\"What's the temperature in Peshawar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8KdxITd8ql_"
      },
      "source": [
        "### Step 2: Adding a weather API call\n",
        "\n",
        "What if we want the function 2 to take the city name and give us the weather for that city.\n",
        "\n",
        "Well we know that Open Weather Map is [integrated](https://python.langchain.com/docs/integrations/tools/openweathermap) into LangChain\n",
        "\n",
        "We need to install pyown, create an API key on the website of Open Weather Map (which takes a few hours to activate) and then run the cells below to get weather of a given city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3vGkGEDO8ql_",
        "outputId": "416394d5-b751-4eb5-e8cd-f04b441c0dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU pyowm langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openweathermap_api_key = userdata.get('OPENWEATHERMAP_API_KEY')"
      ],
      "metadata": {
        "id": "tebgXKuDTnQJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ywMR4xh18ql_"
      },
      "outputs": [],
      "source": [
        "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
        "\n",
        "\n",
        "weather = OpenWeatherMapAPIWrapper(openweathermap_api_key=openweathermap_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "HzdF2hym8ql_",
        "outputId": "cc5ecff0-7d69-45d4-bb99-e582c9edc0bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In London, the current weather is as follows:\n",
            "Detailed status: overcast clouds\n",
            "Wind speed: 2.57 m/s, direction: 20°\n",
            "Humidity: 85%\n",
            "Temperature: \n",
            "  - Current: 4.99°C\n",
            "  - High: 5.63°C\n",
            "  - Low: 4.45°C\n",
            "  - Feels like: 2.83°C\n",
            "Rain: {}\n",
            "Heat index: None\n",
            "Cloud cover: 100%\n"
          ]
        }
      ],
      "source": [
        "weather_data = weather.run(\"London\")\n",
        "print(weather_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnmzZawO8qmA"
      },
      "source": [
        "Now, let's integrate this into function 2 and call the function two as a \"tool\" or \"weather_agent\" instead of \"node_2\" in our workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "n-PJb0Kj8qmA"
      },
      "outputs": [],
      "source": [
        "def function_1(input_1):\n",
        "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
        "        Nothing more, just the city name mentioned. Following is the user query: \" + input_1\n",
        "    response = model.invoke(complete_query)\n",
        "    return response.content\n",
        "\n",
        "def function_2(input_2):\n",
        "    weather_data = weather.run(input_2)\n",
        "    return weather_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "IFUD0pzW8qmA"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph, START, END\n",
        "\n",
        "# Define a graph\n",
        "workflow = Graph()\n",
        "\n",
        "# define nodes\n",
        "workflow.add_node(\"node_1\", function_1)\n",
        "workflow.add_node(\"weather_agent\", function_2)\n",
        "\n",
        "# define edges\n",
        "workflow.add_edge(START, 'node_1')\n",
        "workflow.add_edge('node_1', 'weather_agent')\n",
        "workflow.add_edge('weather_agent', END)\n",
        "\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "zmDJTBUk1kR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "duNZxbAu8qmA",
        "outputId": "dbdf089e-e47c-42bb-efa7-afbc6c156944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> In Las Vegas, the current weather is as follows:\n> Detailed status: few clouds\n> Wind speed: 4.12 m/s, direction: 70°\n> Humidity: 23%\n> Temperature: \n>   - Current: 5.21°C\n>   - High: 7.03°C\n>   - Low: 3.88°C\n>   - Feels like: 2.05°C\n> Rain: {}\n> Heat index: None\n> Cloud cover: 20%"
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "to_markdown(graph.invoke(\"What's the temperature in Las Vegas\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DNiNuZT-8qmA",
        "outputId": "6294b1bb-1d57-4d93-bee5-8c6af58f5854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from node 'node_1':\n",
            "---\n",
            "Las Vegas\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'weather_agent':\n",
            "---\n",
            "In Las Vegas, the current weather is as follows:\n",
            "Detailed status: few clouds\n",
            "Wind speed: 4.12 m/s, direction: 70°\n",
            "Humidity: 23%\n",
            "Temperature: \n",
            "  - Current: 5.21°C\n",
            "  - High: 7.03°C\n",
            "  - Low: 3.88°C\n",
            "  - Feels like: 2.05°C\n",
            "Rain: {}\n",
            "Heat index: None\n",
            "Cloud cover: 20%\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = \"What's the temperature in Las Vegas\"\n",
        "for output in graph.stream(input):\n",
        "    # stream() yields dictionaries with output keyed by node name\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8zTLlyh8qmA"
      },
      "source": [
        "### Step 3 Adding another LLM Call to filter results\n",
        "\n",
        "What if we only want the temperature? But current setup gives us the full weather report.\n",
        "\n",
        "Well we can make another LLM call to filter data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Uu8CJy1b8qmA"
      },
      "outputs": [],
      "source": [
        "def function_3(input_3):\n",
        "    complete_query = \"Your task is to provide info concisely based on the user query. Following is the user query: \" + \"user input\"\n",
        "    response = model.invoke(complete_query)\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D-ZvlOF8qmB"
      },
      "source": [
        "But the issue is the user input is not available from node 2.\n",
        "\n",
        "Can we pass user input all along from first node to the last?\n",
        "\n",
        "Yes, we can use a dictionary and pass it between nodes (we could also use just a list, but dict makes it a bit easier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fsZvoiDd8qmB"
      },
      "outputs": [],
      "source": [
        "# assign AgentState as an empty dict\n",
        "AgentState = {}\n",
        "\n",
        "# messages key will be assigned as an empty array. We will append new messages as we pass along nodes.\n",
        "AgentState[\"messages\"] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZcgSp0KM8qmB",
        "outputId": "833245c3-2883-4664-afa2-9dd67422a4b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': []}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "AgentState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqC8ekXA8qmH"
      },
      "source": [
        "Our goal is to have this state filled as:\n",
        "{'messages': [HumanMessage, AIMessage, ...]]}\n",
        "\n",
        "Also now we need to modify our functions to pass info along the new AgentState"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "I2p11_u18qmI"
      },
      "outputs": [],
      "source": [
        "def function_1(state):\n",
        "    messages = state['messages']\n",
        "    user_input = messages[-1]\n",
        "    complete_query = \"Your task is to provide only the city name based on the user query. \\\n",
        "                    Nothing more, just the city name mentioned. Following is the user query: \" + user_input\n",
        "    response = model.invoke(complete_query)\n",
        "    state['messages'].append(response.content) # appending AIMessage response to the AgentState\n",
        "    return state\n",
        "\n",
        "def function_2(state):\n",
        "    messages = state['messages']\n",
        "    agent_response = messages[-1]\n",
        "    weather = OpenWeatherMapAPIWrapper(openweathermap_api_key=openweathermap_api_key)\n",
        "    weather_data = weather.run(agent_response)\n",
        "    state['messages'].append(weather_data)\n",
        "    return state\n",
        "\n",
        "def function_3(state):\n",
        "    messages = state['messages']\n",
        "    user_input = messages[0]\n",
        "    available_info = messages[-1]\n",
        "    agent2_query = \"Your task is to provide info concisely based on the user query and the available information from the internet. \\\n",
        "                        Following is the user query: \" + user_input + \" Available information: \" + available_info\n",
        "    response = model.invoke(agent2_query)\n",
        "    return response.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uay5ow9k8qmI"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import Graph, START, END\n",
        "\n",
        "# Define a graph\n",
        "workflow = Graph()\n",
        "\n",
        "# define nodes\n",
        "workflow.add_node(\"agent\", function_1)\n",
        "workflow.add_node(\"tool\", function_2)\n",
        "workflow.add_node(\"responder\",function_3)\n",
        "\n",
        "# define edges\n",
        "workflow.add_edge(START, 'agent')\n",
        "workflow.add_edge('agent', 'tool')\n",
        "workflow.add_edge('tool', 'responder')\n",
        "workflow.add_edge('responder', END)\n",
        "\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "mWeQCY7P1nDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph.invoke(\"what is the temperature in las vegas\")"
      ],
      "metadata": {
        "id": "YAETsa9feXa2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "g6W2x7xI8qmI",
        "outputId": "624576fd-420c-402f-b3c0-009ea0d2ec96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature in Las Vegas is 5.18°C, with a high of 7.03°C and a low of 3.88°C. It feels like 2.01°C.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "inputs = {\"messages\": [\"what is the temperature in las vegas\"]}\n",
        "graph.invoke(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "J6OV81OV8qmI",
        "outputId": "01f44530-b852-4f57-a373-0f00d448906f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from node 'agent':\n",
            "---\n",
            "{'messages': ['what is the temperature in las vegas', 'Las Vegas']}\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'tool':\n",
            "---\n",
            "{'messages': ['what is the temperature in las vegas', 'Las Vegas', 'In Las Vegas, the current weather is as follows:\\nDetailed status: few clouds\\nWind speed: 4.12 m/s, direction: 70°\\nHumidity: 23%\\nTemperature: \\n  - Current: 5.18°C\\n  - High: 7.03°C\\n  - Low: 3.88°C\\n  - Feels like: 2.01°C\\nRain: {}\\nHeat index: None\\nCloud cover: 20%']}\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'responder':\n",
            "---\n",
            "The current temperature in Las Vegas is 5.18°C, with a high of 7.03°C and a low of 3.88°C. It feels like 2.01°C.\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "input = {\"messages\": [\"what is the temperature in las vegas\"]}\n",
        "for output in graph.stream(input):\n",
        "    # stream() yields dictionaries with output keyed by node name\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **State**\n",
        "- First, define the State of the graph.\n",
        "\n",
        "- The State schema serves as the input schema for all Nodes and Edges in the graph.\n",
        "\n",
        "Let's use the TypedDict class from python's typing module as our schema, which provides type hints for the keys."
      ],
      "metadata": {
        "id": "g0PnRXCHu6EB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFONdERN8qmI"
      },
      "source": [
        "As we notice that there is a lot of appending to the array going on, we can make it a bit easier with the following:\n",
        "\n",
        "```bash\n",
        "from typing import TypedDict, Annotated\n",
        "from langchain_core.messages import AnyMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bm0wmc28qmI"
      },
      "source": [
        "It basically makes the state dictionary as saw previously, and also makes sure that any new message is appended to the messages array when we do the following:\n",
        "```bash\n",
        "{\"messages\": [new_array_element]}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kybF5kNy8qmI"
      },
      "source": [
        "\n",
        "##### We also realize that our app is not capable of answering simple questions like \"how are you?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "otyNq8S68qmI",
        "outputId": "d568a4da-1831-4365-933e-223572aa9e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Unable to find the resource",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3c9748298396>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   2070\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1725\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    231\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m                 )\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-b64ed4cc9e42>\u001b[0m in \u001b[0;36mfunction_2\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0magent_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mweather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenWeatherMapAPIWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenweathermap_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopenweathermap_api_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mweather_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'messages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweather_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/openweathermap.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, location)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;34m\"\"\"Get the current weather information for a specified location.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweather_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweather_at_place\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyowm/weatherapi25/weather_manager.py\u001b[0m in \u001b[0;36mweather_at_place\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Value must be a string\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOBSERVATION_URI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyowm/commons/http_client.py\u001b[0m in \u001b[0;36mget_json\u001b[0;34m(self, path, params, headers)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'API call timeouted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mHttpClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_status_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyowm/commons/http_client.py\u001b[0m in \u001b[0;36mcheck_status_code\u001b[0;34m(cls, status_code, payload)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnauthorizedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid API Key provided'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unable to find the resource'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadGatewayError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unable to contact the upstream server'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unable to find the resource"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\": [\"hello\"]}\n",
        "graph.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n99OGZk68qmJ"
      },
      "source": [
        "This is because we always want to parse a city and then find the weather.\n",
        "\n",
        "We can make our agent smarter by saying only use the tool when needed, if not just respond back to the user.\n",
        "\n",
        "The way we can do this LangGraph is:\n",
        "1. binding a tool to the agent\n",
        "2. adding a conditional edge to the agent with the option to either call the tool or not\n",
        "3. defining the criteria for the conditional edge as when to call the tool. We will define a function for this.\n",
        "\n",
        "\n",
        "Let's start with the AgentState definition as mentioned a few cells above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "rN7Axltd8qmJ"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import AnyMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "# Define a custom TypedDict that includes a list of messages with add_messages reducer\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kinGzIt8qmJ"
      },
      "source": [
        "Binding tool with agent (LLM Model) is made easy in langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "lhQXndI68qmJ"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.utilities import OpenWeatherMapAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# Initialize an instance of the ChatGoogleGenerativeAI with specific parameters\n",
        "llm: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
        "    # model=\"gemini-1.5-flash\",  # Specify the model to use\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    api_key=gemini_api_key    # Provide the Google API key for authentication\n",
        ")\n",
        "\n",
        "\n",
        "weather = OpenWeatherMapAPIWrapper(openweathermap_api_key=openweathermap_api_key)\n",
        "\n",
        "@tool\n",
        "def get_weather(city: str):\n",
        "    \"\"\"Get the current weather in a given city.\n",
        "\n",
        "    Args:\n",
        "        city: The city to get the weather for.\n",
        "\n",
        "    Returns:\n",
        "        The current weather in the given city.\n",
        "    \"\"\"\n",
        "    return weather.run(city)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [get_weather]\n",
        "llm_with_tool = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "R-CZNsAdG7hz"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tool.invoke(\"What is the temperature in Peshawar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU9Hy05E2MTN",
        "outputId": "174c98e3-df59-4323-d008-cfcdaa6622dd"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"city\": \"Peshawar\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-70424d00-1665-444e-858c-6c2984d3ce31-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Peshawar'}, 'id': '6f207715-db57-438c-89de-54a49d59980f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 50, 'output_tokens': 6, 'total_tokens': 56, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Wrhj-w8qmJ"
      },
      "source": [
        "Our modified function_1 now becomes as below. The reason is, we are passing the human message as state and appending response to the state. Also, our agent now has a tool bound to it, that it can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "58cOE_ID8qmJ"
      },
      "outputs": [],
      "source": [
        "def function_1(state: AgentState):\n",
        "    messages = state['messages']\n",
        "    response = llm_with_tool.invoke(messages)\n",
        "    return {\"messages\": [response]}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now make a tool node"
      ],
      "metadata": {
        "id": "g2C7tzpmCO2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "tools = [get_weather]\n",
        "\n",
        "function_2 = ToolNode(tools)"
      ],
      "metadata": {
        "id": "S7NtR-VSCRLw"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm8nxj4g8qmK"
      },
      "source": [
        "Now with all of the changes above, our LangGraph app is modified as below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "yGTrJ2ey8qmK"
      },
      "outputs": [],
      "source": [
        "# from langgraph.graph import Graph, END\n",
        "\n",
        "# workflow = Graph()\n",
        "\n",
        "# Or you could import StateGraph and pass AgentState to it\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import tools_condition\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", function_1)\n",
        "workflow.add_node(\"tools\", function_2)\n",
        "\n",
        "# The conditional edge requires the following info below.\n",
        "# First, we define the start node. We use `agent`.\n",
        "# This means these are the edges taken after the `agent` node is called.\n",
        "# Next, we pass in the function that will determine which node is called next, in our case where_to_go().\n",
        "\n",
        "workflow.add_edge(START, 'agent')\n",
        "\n",
        "# We now add a conditional edge from `agent` to `tool`.\n",
        "workflow.add_conditional_edges(\n",
        "                \"agent\",\n",
        "                # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "                # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "                tools_condition\n",
        "                )\n",
        "\n",
        "# We now add a normal edge from `tools` to `agent`.\n",
        "# This means that if `tool` is called, then it has to call the 'agent' next.\n",
        "workflow.add_edge('tools', END)\n",
        "\n",
        "# Basically, agent node has the option to call a tool node based on a condition,\n",
        "# whereas tool node must call the agent in all cases based on this setup.\n",
        "\n",
        "\n",
        "graph : CompiledStateGraph = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI8Uz1GY8qmK"
      },
      "source": [
        "We also pass the first message using HumanMessage component available in langchain, makes it easy to differentiate from AIMessage, and FunctionMessage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "0rQiGkG51uEf",
        "outputId": "a330d4da-4cfd-4f43-8f12-28e10e12b99b"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # just another helpter function\n",
        "# import textwrap\n",
        "# from IPython.display import display, Markdown\n",
        "# from langchain.schema import messages\n",
        "\n",
        "# def to_markdown(state)-> Markdown:\n",
        "#     # Extract the content of the last message\n",
        "#     last_message = state['messages'][-1]\n",
        "\n",
        "#     # If the message has a content attribute, use that, else handle gracefully\n",
        "#     text = last_message.content\n",
        "\n",
        "#     text = text.replace(\"•\", \"  *\")\n",
        "#     return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "KGuWP2bHC0Ii"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "oWctGOpt8qmK",
        "outputId": "0a5d7e75-da46-4651-bc2f-e187cb824c1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='what is the temperature in Lahore', additional_kwargs={}, response_metadata={}, id='c076c4e7-c9d3-4362-9b1f-db5b75147f07'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"city\": \"Lahore\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-6081ed9e-b9b8-4802-b9b4-e2bb221765bd-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Lahore'}, 'id': 'e416afc7-6c65-423c-a020-af296511c4ac', 'type': 'tool_call'}], usage_metadata={'input_tokens': 50, 'output_tokens': 6, 'total_tokens': 56, 'input_token_details': {'cache_read': 0}}),\n",
              "  ToolMessage(content='In Lahore, the current weather is as follows:\\nDetailed status: smoke\\nWind speed: 0 m/s, direction: 0°\\nHumidity: 71%\\nTemperature: \\n  - Current: 12.99°C\\n  - High: 12.99°C\\n  - Low: 12.99°C\\n  - Feels like: 12.2°C\\nRain: {}\\nHeat index: None\\nCloud cover: 50%', name='get_weather', id='73fd3e7a-60de-4b8d-818f-9bd22c241ef7', tool_call_id='e416afc7-6c65-423c-a020-af296511c4ac')]}"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in Lahore\")]}\n",
        "# to_markdown(graph.invoke(inputs))\n",
        "graph.invoke(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "ee8kO8-S8qmK",
        "outputId": "bef54a09-b151-4342-fa4c-ea0dc6c4a60c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from node 'agent':\n",
            "---\n",
            "{'messages': [AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_weather', 'arguments': '{\"city\": \"las vegas\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-bd0c303e-f6c3-4cac-bf31-453060048ca6-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'las vegas'}, 'id': '149e38b4-67be-4795-8055-cf67abea79ed', 'type': 'tool_call'}], usage_metadata={'input_tokens': 51, 'output_tokens': 6, 'total_tokens': 57, 'input_token_details': {'cache_read': 0}})]}\n",
            "\n",
            "---\n",
            "\n",
            "Output from node 'tools':\n",
            "---\n",
            "{'messages': [ToolMessage(content='In las vegas, the current weather is as follows:\\nDetailed status: clear sky\\nWind speed: 2.06 m/s, direction: 0°\\nHumidity: 17%\\nTemperature: \\n  - Current: 10.28°C\\n  - High: 11.82°C\\n  - Low: 8.69°C\\n  - Feels like: 7.81°C\\nRain: {}\\nHeat index: None\\nCloud cover: 0%', name='get_weather', id='9c28c2c1-ba9f-4b86-a720-8fe444478afe', tool_call_id='149e38b4-67be-4795-8055-cf67abea79ed')]}\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in las vegas\")]}\n",
        "for output in graph.stream(inputs):\n",
        "    # stream() yields dictionaries with output keyed by node name\n",
        "    for key, value in output.items():\n",
        "        print(f\"Output from node '{key}':\")\n",
        "        print(\"---\")\n",
        "        print(value)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent\n",
        "\n",
        "## Review\n",
        "\n",
        "We built a router.\n",
        "\n",
        "* Our chat model will decide to make a tool call or not based upon the user input\n",
        "* We use a conditional edge to route to a node that will call our tool or simply end\n",
        "\n",
        "![Screenshot 2024-08-21 at 12.44.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0ba0bd34b541c448cc_agent1.png)\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, we can extend this into a generic agent architecture.\n",
        "\n",
        "In the above router, we invoked the model and, if it chose to call a tool, we returned a `ToolMessage` to the user.\n",
        "\n",
        "But, what if we simply pass that `ToolMessage` *back to the model*?\n",
        "\n",
        "We can let it either (1) call another tool or (2) respond directly.\n",
        "\n",
        "This is the intuition behind [ReAct](https://react-lm.github.io/), a general agent architecture.\n",
        "  \n",
        "* `act` - let the model call specific tools\n",
        "* `observe` - pass the tool output back to the model\n",
        "* `reason` - let the model reason about the tool output to decide what to do next (e.g., call another tool or just respond directly)\n",
        "\n",
        "This [general purpose architecture](https://blog.langchain.dev/planning-for-agents/) can be applied to many types of tools.\n",
        "\n",
        "![Screenshot 2024-08-21 at 12.45.43 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac0b4a2c1e5e02f3e78b_agent2.png)"
      ],
      "metadata": {
        "id": "NW95mtAaEOVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langgraph.graph import Graph, END\n",
        "\n",
        "# workflow = Graph()\n",
        "\n",
        "# Or you could import StateGraph and pass AgentState to it\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "tools = [get_weather]\n",
        "workflow.add_node(\"agent\", function_1)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# The conditional edge requires the following info below.\n",
        "# First, we define the start node. We use `agent`.\n",
        "# This means these are the edges taken after the `agent` node is called.\n",
        "# Next, we pass in the function that will determine which node is called next, in our case where_to_go().\n",
        "\n",
        "workflow.add_edge(START, 'agent')\n",
        "\n",
        "# We now add a conditional edge from `agent` to `tool`.\n",
        "workflow.add_conditional_edges(\n",
        "                \"agent\",\n",
        "                # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "                # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "                tools_condition\n",
        "                )\n",
        "\n",
        "# We now add a normal edge from `tools` to `agent`.\n",
        "# This means that if `tool` is called, then it has to call the 'agent' next.\n",
        "workflow.add_edge('tools', \"agent\")\n",
        "\n",
        "# Basically, agent node has the option to call a tool node based on a condition,\n",
        "# whereas tool node must call the agent in all cases based on this setup.\n",
        "\n",
        "\n",
        "react_graph : CompiledStateGraph = workflow.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "7oJibGJp_tES"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"messages\": [HumanMessage(content=\"what is the temperature in Lahore\")]}\n",
        "response = react_graph.invoke(inputs)\n",
        "# to_markdown(response)\n",
        "\n",
        "for m in response[\"messages\"]:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RGdasnE_5G0",
        "outputId": "b629127d-8d6d-4d09-ae66-00c119a4254c"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "what is the temperature in Lahore\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  get_weather (b9623b60-3cd4-43e5-b86e-0e2942888154)\n",
            " Call ID: b9623b60-3cd4-43e5-b86e-0e2942888154\n",
            "  Args:\n",
            "    city: Lahore\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: get_weather\n",
            "\n",
            "In Lahore, the current weather is as follows:\n",
            "Detailed status: smoke\n",
            "Wind speed: 0 m/s, direction: 0°\n",
            "Humidity: 71%\n",
            "Temperature: \n",
            "  - Current: 11.99°C\n",
            "  - High: 11.99°C\n",
            "  - Low: 11.99°C\n",
            "  - Feels like: 11.1°C\n",
            "Rain: {}\n",
            "Heat index: None\n",
            "Cloud cover: 20%\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The temperature in Lahore is currently 11.99°C, but it feels like 11.1°C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvG4mJ6h8qmK"
      },
      "source": [
        "Hopefully, that gives you a good understanding of how we built a LangGraph app and why we used different LC components."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}